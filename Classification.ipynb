{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gy8ZQS_dqVmP",
    "outputId": "d1decd2b-4f6c-4194-b258-570462124599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\richa\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\richa\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\richa\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\richa\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\richa\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\richa\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "     ---------------------------------------- 0.0/126.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/126.0 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/126.0 kB ? eta -:--:--\n",
      "     ------------------ ------------------ 61.4/126.0 kB 656.4 kB/s eta 0:00:01\n",
      "     ------------------------------------ 126.0/126.0 kB 925.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\richa\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\richa\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\richa\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\richa\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\richa\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2023.7.22)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\richa\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install vaderSentiment\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "-bx2GVGbqB3H"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"en_core_web_sm\",  disable=['ner'])\n",
    "data_path = \"/content/drive/MyDrive/Colab Notebooks/JokeRecommendation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "3W-00p2L8YB7"
   },
   "outputs": [],
   "source": [
    "word_list = [\n",
    "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\",\n",
    "    \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\",\n",
    "    \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
    "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\",\n",
    "    \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "    \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\",\n",
    "    \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\",\n",
    "    \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\",\n",
    "    \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\",\n",
    "    \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\",\n",
    "    \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\",\n",
    "    \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\",\n",
    "    \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\",\n",
    "    \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
    "]\n",
    "word_list2=word_list = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\",\n",
    "    \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\", \"around\", \"as\", \"at\", \"back\", \"be\", \"became\",\n",
    "    \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\",\n",
    "    \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\",\n",
    "    \"computer\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\",\n",
    "    \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\",\n",
    "    \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\",\n",
    "    \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\",\n",
    "    \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\",\n",
    "    \"hereupon\", \"hers\", \"herse\", \"him\", \"himse\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\",\n",
    "    \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itse\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\",\n",
    "    \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myse\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\",\n",
    "    \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\",\n",
    "    \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\",\n",
    "    \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\",\n",
    "    \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\",\n",
    "    \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\",\n",
    "    \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\",\n",
    "    \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\",\n",
    "    \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\",\n",
    "    \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\",\n",
    "    \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\",\n",
    "    \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "v6PkZJZOxHrN"
   },
   "outputs": [],
   "source": [
    "ratings_dict = {\n",
    "    1: 0.7113596373160435, 2: 0.5406573329719271, 3: 0.5482025562029306, 4: 0.19582445299922388, 5: 0.5585679956091371,\n",
    "    6: 0.7908864546415826, 7: 0.34661618030779723, 8: 0.26967597089133005, 9: 0.3497724994082854, 10: 0.7652543421973095,\n",
    "    11: 0.8492416208405883, 12: 0.8182238578282195, 13: 0.12511505187728453, 14: 0.7892291145834065, 15: 0.10788722723244895,\n",
    "    16: 0.04297798752495572, 17: 0.21617436057900516, 18: 0.2877856162643668, 19: 0.5427850656877006, 20: 0.23089408680387508,\n",
    "    21: 0.8915627320860312, 22: 0.7070969717133107, 23: 0.5427423568000921, 24: 0.15048448665014053, 25: 0.6030993832580226,\n",
    "    26: 0.7697969357291401, 27: 0.9482399523268288, 28: 0.8010165506479477, 29: 0.9321066868009251, 30: 0.3835907625529182,\n",
    "    31: 0.8757784591586277, 32: 0.9514178833828416, 33: 0.19998414339764875, 34: 0.7154218834529914, 35: 0.9413433158825601,\n",
    "    36: 0.9486923628269315, 37: 0.19634645758449448, 38: 0.7939166474422212, 39: 0.750382900511494, 40: 0.737619178351302,\n",
    "    41: 0.40696097305536166, 42: 0.8509092268469134, 43: 0.30272048766160303, 44: 0.11938093813049189, 45: 0.737975605299451,\n",
    "    46: 0.7815188127943004, 47: 0.7921551362389441, 48: 0.8549766858569523, 49: 0.9252155396173509, 50: 0.9665147960588806,\n",
    "    51: 0.3064193590499113, 52: 0.44988921117708286, 53: 0.9275895799942858, 54: 0.9172669603415716, 55: 0.5944638808586404,\n",
    "    56: 0.8437015531668218, 57: 0.12555894752975108, 58: 0.024020334922397083, 59: 0.372430386373687, 60: 0.39967603673360663,\n",
    "    61: 0.8900076421245693, 62: 0.9356113778156165, 63: 0.5635556484127066, 64: 0.32809504380714183, 65: 0.8797901373405882,\n",
    "    66: 0.920055334760205, 67: 0.2873521242205152, 68: 0.9177053717290367, 69: 0.9071574235735527, 70: 0.6282500031439525,\n",
    "    71: 0.27232835290184676, 72: 0.9329192447359996, 73: 0.7246146400286528, 74: 0.17389947475044995, 75: 0.3900402381719742,\n",
    "    76: 0.9124886827943671, 77: 0.6556108039200406, 78: 0.8397499151703745, 79: 0.4638188927105016, 80: 0.7362371140669255,\n",
    "    81: 0.8517292243013638, 82: 0.6513263540793754, 83: 0.8775023163154674, 84: 0.6281606864756035, 85: 0.6527676661684987,\n",
    "    86: 0.5671678000774013, 87: 0.8643060469432947, 88: 0.8771245714578242, 89: 0.966040142363291, 90: 0.5913096286283207,\n",
    "    91: 0.8706902468389563, 92: 0.7543190802777354, 93: 0.9047038263431125, 94: 0.7470544700016736, 95: 0.7378804512444108,\n",
    "    96: 0.7978687168912821, 97: 0.8159440838722309, 98: 0.7047174743883199, 99: 0.490953742994065, 100: 0.7533969619731737,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Y4hlFvPRqLV4"
   },
   "outputs": [],
   "source": [
    "# creds: https://github.com/pemagrg1/Text-Pre-Processing-in-Python/blob/master/individual_python_files/expanding_contractions.py\n",
    "contractions_dict = {\n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I had\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"iit will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so is\",\n",
    "\"that'd\": \"that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they had\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text):\n",
    "  contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
    "                                    flags=re.IGNORECASE | re.DOTALL)\n",
    "  def expand_match(contraction):\n",
    "      match = contraction.group(0)\n",
    "      first_char = match[0]\n",
    "      expanded_contraction = contractions_dict.get(match) \\\n",
    "          if contractions_dict.get(match) \\\n",
    "          else contractions_dict.get(match.lower())\n",
    "      expanded_contraction = expanded_contraction\n",
    "      return expanded_contraction\n",
    "\n",
    "  expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "  expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "  return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "baJbinckqPWc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "joke_path = os.path.join(data_path,\"/content/jokes.csv\")\n",
    "df = pd.read_csv(joke_path)\n",
    "\n",
    "abbr={\n",
    "    \"Q>\":\"Question \",\n",
    "    \"Q.\":\"Question \",\n",
    "    \"Q:\":\"Question \",\n",
    "    \"A:\":\"Answer \",\n",
    "    \"A.\":\"Answer \",\n",
    "}\n",
    "def create_strings_with_overlap(paragraph, overlap=3, words_per_string=7):\n",
    "    words = paragraph.split()\n",
    "    result_strings = []\n",
    "    for i in range(0, len(words) - words_per_string + 1, words_per_string - overlap):\n",
    "        result_strings.append(' '.join(words[i:i+words_per_string]))\n",
    "    # Include the remaining words at the end\n",
    "    remaining_words = words[-(words_per_string-overlap):]\n",
    "    if remaining_words:\n",
    "        result_strings.append(' '.join(remaining_words))\n",
    "    return result_strings\n",
    "\n",
    "def remove_abbreviations(text):\n",
    "  #print(\"abbr before: \",text)\n",
    "  for key, value in abbr.items():\n",
    "    pattern = re.compile(key)\n",
    "    text = pattern.sub(value, text)\n",
    "  #print(\"abbr AFTER: \",text)\n",
    "  return text\n",
    "\n",
    "def remove_abbreviations1(text):\n",
    "    pattern = re.compile(r'A[.:>]')\n",
    "    text = pattern.sub(\"Answer\", text)\n",
    "    pattern2 = re.compile(r'Q[.:>]')\n",
    "    text = pattern2.sub(\"Question\", text)\n",
    "    return text\n",
    "\n",
    "def tokenizeSentence(text):\n",
    "  return sent_tokenize(text)\n",
    "\n",
    "def remove_slashes(text):\n",
    "   cleaned_sentence = re.sub(r'[/]', '', text)\n",
    "   return cleaned_sentence\n",
    "\n",
    "def remove_special_chars(text):\n",
    "  #print(\"sp before: \",text)\n",
    "  pattern = r'[^A-Za-z0-9.\\s]'\n",
    "  cleaned_text = re.sub(pattern, '', text)\n",
    "  #print(\"sp after: \",cleaned_text)\n",
    "  return cleaned_text\n",
    "\n",
    "def remove_periods(sentence):\n",
    "  #print(\"rp before: \",sentence)\n",
    "  cleaned_sentence = re.sub(r'\\s*\\.\\s*$', '', sentence)\n",
    "  cleaned_sentence = re.sub(r'(\\w)\\s*\\.\\s*(\\w)', r'\\1 \\2', cleaned_sentence)\n",
    "  #print(\"rp after: \",cleaned_sentence)\n",
    "  return cleaned_sentence\n",
    "\n",
    "def remove_period2(sentence):\n",
    "  #print(\"rp2 before: \",sentence)\n",
    "  cleaned_sentence = re.sub(r'[.]', ' ', sentence)\n",
    "  #print(\"rp2 after: \",cleaned_sentence)\n",
    "  return cleaned_sentence\n",
    "\n",
    "def remove_stopwords_custom(text):\n",
    "  pattern = re.compile(r'\\b(?:' + '|'.join(word_list) + r')\\b', flags=re.IGNORECASE)\n",
    "  result_text = pattern.sub('', text)\n",
    "  return result_text\n",
    "\n",
    "def remove_stop_words_spacy(text):\n",
    "  #print(\"stopword before: \",text)\n",
    "  doc = nlp(text)\n",
    "  cleaned_text = ' '.join(token.text for token in doc if not token.is_stop)\n",
    "  #print(\"stopword after: \",cleaned_text)\n",
    "  return cleaned_text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "  doc = nlp(text)\n",
    "  lemmatized_text = ' '.join(token.lemma_ for token in doc)\n",
    "  return lemmatized_text\n",
    "\n",
    "def remove_multiple_spaces(text):\n",
    "  #print(\"multispace before: \",text)\n",
    "  cleaned_string = re.sub(r'\\s+', ' ', text)\n",
    "  return cleaned_string\n",
    "\n",
    "def preprocess_text_data(text):\n",
    "  text=text.lower()\n",
    "  text=remove_abbreviations1(text)\n",
    "  text=remove_slashes(text)\n",
    "  text=expand_contractions(text)\n",
    "  #print(text)\n",
    "  text=remove_special_chars(text)\n",
    "  text=remove_periods(text)\n",
    "  text=remove_period2(text)\n",
    "  text=remove_stopwords_custom(text)\n",
    "  text=lemmatize_text(text)\n",
    "  text=remove_multiple_spaces(text).strip()\n",
    "  text=str(text).strip().lower()\n",
    "  return text\n",
    "\n",
    "#df['processed_spacy'] = df['Text'].apply(preprocess_text_data)\n",
    "processed_query=[]\n",
    "joke_label=[]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    joke_number = row['JokeId']\n",
    "    text = row['Joke']\n",
    "    #user_rating = row['Joke']\n",
    "    #cleaned_text_nltk = row['CleanedTextNLTK']\n",
    "    #spacy_processed = row[\"processed_spacy\"]\n",
    "    # uttr_list = tokenizeSentence(text)\n",
    "    # for sen in uttr_list:\n",
    "    #   sen=remove_abbreviations1(sen)\n",
    "    #   ptext=preprocess_text_data(sen)\n",
    "    #   joke_label.append(joke_number)\n",
    "    #   processed_query.append(ptext)\n",
    "    text=preprocess_text_data(text)\n",
    "    overlap_str_list = create_strings_with_overlap(text)\n",
    "    for overlap_str in overlap_str_list:\n",
    "      ptext=str(overlap_str).strip().lower()\n",
    "      joke_label.append(joke_number)\n",
    "      processed_query.append(ptext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "HhGj2petqTkX"
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(list(zip(joke_label, processed_query)), columns=['JokeId', 'JokeSentences'])\n",
    "df1.to_csv(\"processed_jokes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CCkwN2wEsStI",
    "outputId": "0095bb64-8076-40e0-9ad3-447dec36f98c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['00', '00 ask', '00 ask come', '00 month', '00 month asian',\n",
       "       '00 teller', '00 teller say', '1', '1 age', '1 age 13',\n",
       "       ...\n",
       "       'young man see', 'young scottish', 'young scottish gentleman', 'zipper',\n",
       "       'zipper button', 'zipper button velcro', 'zool', 'zool small',\n",
       "       'zool small asiatic', 'sentiment'],\n",
       "      dtype='object', length=6005)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "#ngram_features = vectorizer.fit_transform(df['processed_spacy']).toarray()\n",
    "#ngram_df = pd.DataFrame(ngram_features, columns=vectorizer.get_feature_names_out())\n",
    "def tokenizer(text):\n",
    "  doc = nlp(text)\n",
    "  tokens = [token.text for token in doc]\n",
    "  return tokens\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3),tokenizer = tokenizer)\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(processed_query).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_features, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "sentiment_score=[]\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for sent in processed_query:\n",
    "  sentiment_score.append(sid.polarity_scores(sent)['compound'])\n",
    "sen_df = pd.DataFrame(sentiment_score, columns=['sentiment'])\n",
    "\n",
    "#featurized_df = pd.concat([ngram_df, tfidf_df, df['VaderSentiment']], axis=1)\n",
    "featurized_df = pd.concat([tfidf_df, sen_df['sentiment']], axis=1)\n",
    "\n",
    "featurized_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "NBA3-N3zBgjz"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "joke_path = os.path.join(data_path,\"jokes\")\n",
    "filename_pattern = re.compile(r'init(\\d+)\\.html')\n",
    "\n",
    "def extract_text_from_html():\n",
    "    jokes={}\n",
    "    for filename in os.listdir(joke_path):\n",
    "      if filename.endswith('.html'):\n",
    "          matched = filename_pattern.match(filename)\n",
    "          if matched:\n",
    "            number = int(matched.group(1))\n",
    "          file_path = os.path.join(joke_path, filename)\n",
    "          with open(file_path, 'r', encoding='utf-8') as file:\n",
    "             # print(file_path)\n",
    "              soup = BeautifulSoup(file, 'html.parser')\n",
    "              #title_tag = soup.find('title')\n",
    "              body_text = soup.find('body').get_text() if soup.find('body') else \"No body text found\"\n",
    "              body_text=body_text.replace(\"\\n\",\"\")\n",
    "              #tem_dict[\"id\"]=file_path\n",
    "              #tem_dict[\"text\"]=body_text\n",
    "              #joke_list.append(tem_dict)\n",
    "              jokes[number]=body_text\n",
    "    return jokes\n",
    "\n",
    "joke_dict=extract_text_from_html()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZTLeDfjmERb",
    "outputId": "0c4a36e0-d8d3-42fe-99e3-00c6f5e0531c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What did one ocean say to the', 'say to the other ocean? Nothing, they', 'Nothing, they just waved.']\n"
     ]
    }
   ],
   "source": [
    "print(create_strings_with_overlap(\"What did one ocean say to the other ocean? Nothing, they just waved.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLpgJh7OnjX-"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MY-nFSqxKOOx"
   },
   "outputs": [],
   "source": [
    "joke_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "IvvusO90ss8_"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file_path = 'jokes.csv'\n",
    "\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = ['JokeId', 'Joke']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for joke_id, joke_text in joke_dict.items():\n",
    "        writer.writerow({'JokeId': joke_id, 'Joke': joke_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ID8Xjs4isylC",
    "outputId": "86a57d3c-51ab-49b8-ecc1-f93fb08bae81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(joke_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YlhpMQXnuhhY",
    "outputId": "8a9bccd3-8c16-4618-df8b-dda5d8e35b53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7022900763358778\n",
      "F1 Score: 0.7208287895310795\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pickle\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "jk2 = pd.read_csv(\"/content/processed_jokes.csv\")\n",
    "X = jk2['JokeSentences']\n",
    "y = jk2['JokeId']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and Linear SVM\n",
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1, 3))),\n",
    "    ('classifier', SVC(kernel='linear', random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # Specify 'weighted' for multi-class classification\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "iyYr-srOv5fW"
   },
   "outputs": [],
   "source": [
    "# Save TF-IDF vectorizer to a pickle file\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as vectorizer_file:\n",
    "    pickle.dump(model.named_steps['tfidf'], vectorizer_file)\n",
    "\n",
    "# Save SVM model to a pickle file\n",
    "with open('svm_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model.named_steps['classifier'], model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lB72FM4Tv9p3",
    "outputId": "4a3ba95e-d801-40de-8d16-f0d14659117b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels and Ratings:\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 87, Rating: 0.8643060469432947\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 47, Rating: 0.7921551362389441\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 68, Rating: 0.9177053717290367\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 68, Rating: 0.9177053717290367\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 68, Rating: 0.9177053717290367\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 47, Rating: 0.7921551362389441\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 63, Rating: 0.5635556484127066\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 93, Rating: 0.9047038263431125\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 32, Rating: 0.9514178833828416\n",
      "Label: 54, Rating: 0.9172669603415716\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 65, Rating: 0.8797901373405882\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 87, Rating: 0.8643060469432947\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 47, Rating: 0.7921551362389441\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 32, Rating: 0.9514178833828416\n",
      "Label: 53, Rating: 0.9275895799942858\n",
      "Label: 36, Rating: 0.9486923628269315\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "vec_path = \"/content/tfidf_vectorizer.pkl\"\n",
    "svm_path = \"/content/svm_model.pkl\"\n",
    "# Load the TF-IDF vectorizer from the pickle file\n",
    "with open('tfidf_vectorizer.pkl', 'rb') as vectorizer_file:\n",
    "    loaded_vectorizer = pickle.load(vectorizer_file)\n",
    "\n",
    "# Load the SVM model from the pickle file\n",
    "with open('svm_model.pkl', 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)\n",
    "\n",
    "# Function to predict JokeId for a given text joke\n",
    "def predict_joke_id(text_joke):\n",
    "    text_joke=preprocess_text_data(text_joke)\n",
    "    # Use the loaded vectorizer to transform the input text\n",
    "    vectorized_text = loaded_vectorizer.transform([text_joke])\n",
    "\n",
    "    # Use the loaded model to make predictions\n",
    "    predicted_joke_id = loaded_model.predict(vectorized_text)[0]\n",
    "    return predicted_joke_id\n",
    "\n",
    "# Function to get labels and ratings of jokes from a CSV file\n",
    "def get_labels_and_ratings_from_csv(csv_path, ratings_dict):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Assuming the text column in the CSV is named 'Text'\n",
    "    text_column = df['Text']\n",
    "\n",
    "    # Predict JokeId for each text and store the results\n",
    "    results = []\n",
    "    for text in text_column:\n",
    "        predicted_label = predict_joke_id(text)\n",
    "        ratings = ratings_dict.get(predicted_label, \"Rating not available\")\n",
    "        results.append((predicted_label, ratings))\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "csv_path_to_predict = \"/content/drive/MyDrive/Colab Notebooks/JokeRecommendation/test_joke_text.csv\"  # Replace with the actual path\n",
    "predicted_results = get_labels_and_ratings_from_csv(csv_path_to_predict, ratings_dict)\n",
    "\n",
    "print(\"Predicted Labels and Ratings:\")\n",
    "for label, rating in predicted_results:\n",
    "    print(f\"Label: {label}, Rating: {rating}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
